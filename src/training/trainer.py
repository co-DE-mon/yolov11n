import os
from pathlib import Path
from typing import Union


import torch
from copy import deepcopy
from datetime import datetime

from ultralytics import YOLO, __version__

# from ultralytics.nn.tasks import attempt_load_one_weight
from ultralytics.engine.trainer import BaseTrainer

from ultralytics.utils import YAML, LOGGER, RANK, DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS
from ultralytics.utils.checks import check_yaml
from ultralytics.utils.torch_utils import unwrap_model


def save_model_v2(self: BaseTrainer):
    """
    Disabled half precision saving. originated from ultralytics/yolo/engine/trainer.py
    """
    ckpt = {
        "epoch": self.epoch,
        "best_fitness": self.best_fitness,
        "model": deepcopy(unwrap_model(self.model)),  # unwrap DP/DDP/compiled wrappers
        "ema": deepcopy(self.ema.ema),
        "updates": self.ema.updates,
        "optimizer": self.optimizer.state_dict(),
        "train_args": vars(self.args),  # save as dict
        "date": datetime.now().isoformat(),
        "version": __version__,
    }

    # Save last, best and delete
    torch.save(ckpt, self.last)
    if self.best_fitness == self.fitness:
        torch.save(ckpt, self.best)
    if (
        (self.epoch > 0)
        and (self.save_period > 0)
        and (self.epoch % self.save_period == 0)
    ):
        torch.save(ckpt, self.wdir / f"epoch{self.epoch}.pt")
    del ckpt


def final_eval_v2(self: BaseTrainer):
    """
    originated from ultralytics/yolo/engine/trainer.py
    """
    for f in self.last, self.best:
        if f.exists():
            strip_optimizer_v2(f)  # strip optimizers
            if f is self.best:
                LOGGER.info(f"\nValidating {f}...")
                self.metrics = self.validator(model=f)
                self.metrics.pop("fitness", None)
                self.run_callbacks("on_fit_epoch_end")


def strip_optimizer_v2(f: Union[str, Path] = "best.pt", s: str = "") -> None:
    """
    Disabled half precision saving. originated from ultralytics/yolo/utils/torch_utils.py
    """
    x = torch.load(f, weights_only=False)
    args = {
        **DEFAULT_CFG_DICT,
        **x["train_args"],
    }  # combine model args with default args, preferring model args
    if x.get("ema"):
        x["model"] = x["ema"]  # replace model with ema
    for k in "optimizer", "ema", "updates":  # keys
        x[k] = None
    for p in x["model"].parameters():
        p.requires_grad = False
    x["train_args"] = {
        k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS
    }  # strip non-default keys
    # x['model'].args = x['train_args']
    torch.save(x, s or f)
    mb = os.path.getsize(s or f) / 1e6  # filesize
    LOGGER.info(
        f"Optimizer stripped from {f},{f' saved as {s},' if s else ''} {mb:.1f}MB"
    )


def train_v2(self: YOLO, pruning=False, **kwargs):
    """
    Disabled loading new model when pruning flag is set. originated from ultralytics/yolo/engine/model.py
    """

    self._check_is_pytorch_model()
    if self.session:  # Ultralytics HUB session
        if any(kwargs):
            LOGGER.warning(
                "WARNING ⚠️ using HUB training arguments, ignoring local training arguments."
            )
        kwargs = self.session.train_args

    overrides = self.overrides.copy()
    overrides.update(kwargs)

    if kwargs.get("cfg"):
        LOGGER.info(f"cfg file passed. Overriding default params with {kwargs['cfg']}.")
        overrides = YAML.load(check_yaml(kwargs["cfg"]))

    overrides["mode"] = "train"
    if not overrides.get("data"):
        raise AttributeError(
            "Dataset required but missing, i.e. pass 'data=coco128.yaml'"
        )
    if overrides.get("resume"):
        overrides["resume"] = self.ckpt_path

    if pruning:
        model_size = overrides.get("model_size", "n")
        model_family = overrides.get("model_family", "yolo11")
        overrides["model"] = f"{model_family}{model_size}.pt"

    self.task = overrides.get("task") or self.task
    task_map = self.task_map
    self.trainer = task_map[self.task]["trainer"](
        overrides=overrides, _callbacks=self.callbacks
    )

    if not pruning:
        if not overrides.get("resume"):  # manually set model only if not resuming
            self.trainer.model = self.trainer.get_model(
                weights=self.model if self.ckpt else None, cfg=self.model.yaml
            )
            self.model = self.trainer.model

    else:
        # pruning mode
        self.trainer.pruning = True
        self.trainer.model = self.model

        # replace some functions to disable half precision saving
        self.trainer.save_model = save_model_v2.__get__(self.trainer)
        self.trainer.final_eval = final_eval_v2.__get__(self.trainer)

    self.trainer.hub_session = self.session  # attach optional HUB session
    self.trainer.train()
    # Update model and cfg after training
    if RANK in (-1, 0):
        # self.model, _ = attempt_load_one_weight(str(self.trainer.best))
        loaded = YOLO(str(self.trainer.best))
        self.model = loaded.model
        self.overrides = self.model.args
        self.metrics = getattr(self.trainer.validator, "metrics", None)
